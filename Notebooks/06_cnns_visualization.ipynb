{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_cnns_visualization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUKImo52W8vdqseaNeD5Pb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlelarge/dataflowr/blob/master/Notebooks/06_cnns_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An7MUVlT63b3",
        "colab_type": "text"
      },
      "source": [
        "Code adapted from repo by [Utku Ozbulak](https://github.com/utkuozbulak/pytorch-cnn-visualizations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFBYfFMZho01",
        "colab_type": "text"
      },
      "source": [
        "# CNN Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDkDh4FZhkX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.cm as mpl_color_map\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViHVpzFQhs7Y",
        "colab_type": "text"
      },
      "source": [
        "### Some misc functions that we will use across methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvzYpKFMhq56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im\n",
        "\n",
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../generated'):\n",
        "        os.makedirs('../generated')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../generated', file_name + '.jpg')\n",
        "    save_image(gradient, path_to_file)\n",
        "    \n",
        "def save_class_activation_images(org_img, activation_map, file_name):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        file_name (str): File name of the exported image\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../generated'):\n",
        "        os.makedirs('../generated')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    # Save colored heatmap\n",
        "    path_to_file = os.path.join('../generated', file_name+'_Cam_Heatmap.png')\n",
        "    save_image(heatmap, path_to_file)\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('../generated', file_name+'_Cam_On_Image.png')\n",
        "    save_image(heatmap_on_image, path_to_file)\n",
        "    # Save grayscale heatmap\n",
        "    path_to_file = os.path.join('../generated', file_name+'_Cam_Grayscale.png')\n",
        "    save_image(activation_map, path_to_file)\n",
        "\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on iamge\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "\n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "\n",
        "    Args:\n",
        "        PIL_img (PIL_img): Image to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im.thumbnail((224, 224))\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "\n",
        "def get_positive_negative_saliency(gradient):\n",
        "    \"\"\"\n",
        "        Generates positive and negative saliency maps based on the gradient\n",
        "    Args:\n",
        "        gradient (numpy arr): Gradient of the operation to visualize\n",
        "\n",
        "    returns:\n",
        "        pos_saliency ( )\n",
        "    \"\"\"\n",
        "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
        "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
        "    return pos_saliency, neg_saliency\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqoH6QzhwBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        file_name_to_export (string): File name to export the visualizations\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    img_url = 'https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/'\n",
        "    # Pick one of the examples\n",
        "    example_list = ((f'{img_url}snake.jpg', 56),\n",
        "                    (f'{img_url}cat_dog.png', 243),\n",
        "                    (f'{img_url}spider.png', 72))\n",
        "    img_path = example_list[example_index][0]\n",
        "    target_class = example_list[example_index][1]\n",
        "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
        "    # Read image\n",
        "    response = requests.get(img_path)\n",
        "    original_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    pretrained_model = models.alexnet(pretrained=True)\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class,\n",
        "            file_name_to_export,\n",
        "            pretrained_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCUQ0O4Ih01I",
        "colab_type": "text"
      },
      "source": [
        "## Gradient visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFA5JtPihyDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class VanillaBackprop():\n",
        "    \"\"\"\n",
        "        Produces gradients generated with vanilla back propagation from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        # Hook the first layer to get the gradient\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFL2gx_ksM3z",
        "colab_type": "text"
      },
      "source": [
        "You can find more class ids and names from ImageNet [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwyRL_nh0C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get params\n",
        "target_example = 1 \n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "# Vanilla backprop\n",
        "VBP = VanillaBackprop(pretrained_model)\n",
        "# Generate gradients\n",
        "vanilla_grads = VBP.generate_gradients(prep_img, target_class)\n",
        "# Save colored gradients\n",
        "save_gradient_images(vanilla_grads, file_name_to_export + '_Vanilla_BP_color')\n",
        "# Convert to grayscale\n",
        "grayscale_vanilla_grads = convert_to_grayscale(vanilla_grads)\n",
        "# Save grayscale gradients\n",
        "save_gradient_images(grayscale_vanilla_grads, file_name_to_export + '_Vanilla_BP_gray')\n",
        "print('Vanilla backprop completed')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ouXCxe7sSkH",
        "colab_type": "text"
      },
      "source": [
        "### Smoothened gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_sGDFEfh7MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_multiplier):\n",
        "    \"\"\"\n",
        "        Generates smooth gradients of given Backprop type. You can use this with both vanilla\n",
        "        and guided backprop\n",
        "    Args:\n",
        "        Backprop (class): Backprop type\n",
        "        prep_img (torch Variable): preprocessed image\n",
        "        target_class (int): target class of imagenet\n",
        "        param_n (int): Amount of images used to smooth gradient\n",
        "        param_sigma_multiplier (int): Sigma multiplier when calculating std of noise\n",
        "    \"\"\"\n",
        "    # Generate an empty image/matrix\n",
        "    smooth_grad = np.zeros(prep_img.size()[1:])\n",
        "\n",
        "    mean = 0\n",
        "    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()\n",
        "    for x in range(param_n):\n",
        "        # Generate noise\n",
        "        noise = prep_img.data.new(prep_img.size()).normal_(mean, sigma**2)\n",
        "        # Add noise to the image\n",
        "        noisy_img = prep_img + noise\n",
        "        # Calculate gradients\n",
        "        vanilla_grads = Backprop.generate_gradients(noisy_img, target_class)\n",
        "        # Add gradients to smooth_grad\n",
        "        smooth_grad = smooth_grad + vanilla_grads\n",
        "    # Average it out\n",
        "    smooth_grad = smooth_grad / param_n\n",
        "    return smooth_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZUWaqsxsTEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get params\n",
        "target_example = 1 \n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "VBP = VanillaBackprop(pretrained_model)\n",
        "# change the parametre in generate_smooth_grad\n",
        "\n",
        "param_n = 50\n",
        "param_sigma_multiplier = 4\n",
        "smooth_grad = generate_smooth_grad(VBP, \n",
        "                                   prep_img,\n",
        "                                   target_class,\n",
        "                                   param_n,\n",
        "                                   param_sigma_multiplier)\n",
        "\n",
        "# Save colored gradients\n",
        "save_gradient_images(smooth_grad, file_name_to_export + '_SmoothGrad_color')\n",
        "# Convert to grayscale\n",
        "grayscale_smooth_grad = convert_to_grayscale(smooth_grad)\n",
        "# Save grayscale gradients\n",
        "save_gradient_images(grayscale_smooth_grad, file_name_to_export + '_SmoothGrad_gray')\n",
        "print('Smooth grad completed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrSqQ0Z2sXME",
        "colab_type": "text"
      },
      "source": [
        "## CNN Layer visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzF_9LIsXpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNLayerVisualization():\n",
        "    \"\"\"\n",
        "        Produces an image that minimizes the loss of a convolution\n",
        "        operation for a specific layer and filter\n",
        "    \"\"\"\n",
        "    def __init__(self, model, selected_layer, selected_filter):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.selected_layer = selected_layer\n",
        "        self.selected_filter = selected_filter\n",
        "        self.conv_output = 0\n",
        "        # Create the folder to export images if not exists\n",
        "        if not os.path.exists('../generated'):\n",
        "            os.makedirs('../generated')\n",
        "\n",
        "    def hook_layer(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            # Gets the conv output of the selected filter (from selected layer)\n",
        "            self.conv_output = grad_out[0, self.selected_filter]\n",
        "        # Hook the selected layer\n",
        "        self.model[self.selected_layer].register_forward_hook(hook_function)\n",
        "\n",
        "    def visualise_layer(self):\n",
        "        # Hook the selected layer\n",
        "        self.hook_layer()\n",
        "        # Generate a random image\n",
        "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
        "        # Process image and return variable\n",
        "        processed_image = preprocess_image(random_image, False)\n",
        "        # Define optimizer for the image\n",
        "        optimizer = torch.optim.Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
        "        for i in range(1, 31):\n",
        "            optimizer.zero_grad()\n",
        "            # Assign create image to a variable to move forward in the model\n",
        "            x = processed_image\n",
        "            for index, layer in enumerate(self.model):\n",
        "                # Forward pass layer by layer\n",
        "                # x is not used after this point because it is only needed to trigger\n",
        "                # the forward hook function\n",
        "                x = layer(x)\n",
        "                # Only need to forward until the selected layer is reached\n",
        "                if index == self.selected_layer:\n",
        "                    # (forward hook function triggered)\n",
        "                    break\n",
        "            # Loss function is the mean of the output of the selected layer/filter\n",
        "            # We try to minimize the mean of the output of that specific filter\n",
        "            #########\n",
        "            # TODO: add your loss here: \n",
        "\n",
        "            #########\n",
        "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "            # Recreate image\n",
        "            self.created_image = recreate_image(processed_image)\n",
        "            # Save image\n",
        "            if i % 5 == 0:\n",
        "                im_path = '../generated/layer_vis_l' + str(self.selected_layer) + \\\n",
        "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
        "                save_image(self.created_image, im_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjvgzEKdsahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_layer = 17\n",
        "filter_pos = 5\n",
        "# Fully connected layer is not needed\n",
        "pretrained_model = models.vgg16(pretrained=True).features\n",
        "layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
        "\n",
        "# Layer visualization with pytorch hooks\n",
        "layer_vis.visualise_layer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOcKsMwAscJA",
        "colab_type": "text"
      },
      "source": [
        "## Deep dream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-hh2unYscnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DeepDream():\n",
        "    \"\"\"\n",
        "        Produces an image that minimizes the loss of a convolution\n",
        "        operation for a specific layer and filter\n",
        "    \"\"\"\n",
        "    def __init__(self, model, selected_layer, selected_filter, im_path):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.selected_layer = selected_layer\n",
        "        self.selected_filter = selected_filter\n",
        "        self.conv_output = 0\n",
        "        # Generate a random image\n",
        "        response = requests.get(im_path)\n",
        "        self.created_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "#         self.created_image = Image.open(im_path).convert('RGB')\n",
        "        # Hook the layers to get result of the convolution\n",
        "        self.hook_layer()\n",
        "        # Create the folder to export images if not exists\n",
        "        if not os.path.exists('../generated'):\n",
        "            os.makedirs('../generated')\n",
        "\n",
        "    def hook_layer(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            # Gets the conv output of the selected filter (from selected layer)\n",
        "            self.conv_output = grad_out[0, self.selected_filter]\n",
        "\n",
        "        # Hook the selected layer\n",
        "        self.model[self.selected_layer].register_forward_hook(hook_function)\n",
        "\n",
        "    def dream(self):\n",
        "        # Process image and return variable\n",
        "        self.processed_image = preprocess_image(self.created_image, True)\n",
        "        # Define optimizer for the image\n",
        "        # Earlier layers need higher learning rates to visualize whereas layer layers need less\n",
        "        optimizer = torch.optim.SGD([self.processed_image], lr=12,  weight_decay=1e-4)\n",
        "        for i in range(101):\n",
        "            optimizer.zero_grad()\n",
        "            # Assign create image to a variable to move forward in the model\n",
        "            x = self.processed_image\n",
        "            for index, layer in enumerate(self.model):\n",
        "                # Forward\n",
        "                x = layer(x)\n",
        "                # Only need to forward until we the selected layer is reached\n",
        "                if index == self.selected_layer:\n",
        "                    break\n",
        "            # Loss function is the mean of the output of the selected layer/filter\n",
        "            # We try to minimize the mean of the output of that specific filter\n",
        "            #########\n",
        "            # TODO: add your loss here: \n",
        "\n",
        "            #########\n",
        "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "            # Recreate image\n",
        "            self.created_image = recreate_image(self.processed_image)\n",
        "            # Save image every 20 iteration\n",
        "            if i % 20 == 0:\n",
        "                print(self.created_image.shape)\n",
        "                im_path = '../generated/ddream_l' + str(self.selected_layer) + \\\n",
        "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
        "                save_image(self.created_image, im_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpti39KsezU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_layer = 34\n",
        "filter_pos = 94\n",
        "\n",
        "im_path = 'https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/dd_tree.jpg'\n",
        "\n",
        "pretrained_model = models.vgg19(pretrained=True).features\n",
        "dd = DeepDream(pretrained_model, cnn_layer, filter_pos, im_path)\n",
        "# This operation can also be done without Pytorch hooks\n",
        "# See layer visualisation for the implementation without hooks\n",
        "dd.dream()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7R1TuLsgN9",
        "colab_type": "text"
      },
      "source": [
        "## Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9gzgV-TshJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CamExtractor():\n",
        "    \"\"\"\n",
        "        Extracts cam features from the model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward_pass_on_convolutions(self, x):\n",
        "        \"\"\"\n",
        "            Does a forward pass on convolutions, hooks the function at given layer\n",
        "        \"\"\"\n",
        "        conv_output = None\n",
        "        for module_pos, module in self.model.features._modules.items():\n",
        "            x = module(x)  # Forward\n",
        "            if int(module_pos) == self.target_layer:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                conv_output = x  # Save the convolution output on that layer\n",
        "        return conv_output, x\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        \"\"\"\n",
        "            Does a full forward pass on the model\n",
        "        \"\"\"\n",
        "        # Forward pass on the convolutions\n",
        "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        # Forward pass on the classifier\n",
        "        x = self.model.classifier(x)\n",
        "        return conv_output, x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cQFYtGJsijj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradCam():\n",
        "    \"\"\"\n",
        "        Produces class activation map\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        # Define extractor\n",
        "        self.extractor = CamExtractor(self.model, target_layer)\n",
        "\n",
        "    def generate_cam(self, input_image, target_class=None):\n",
        "        # Full forward pass\n",
        "        # conv_output is the output of convolutions at specified layer\n",
        "        # model_output is the final output of the model (1, 1000)\n",
        "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
        "        if target_class is None:\n",
        "            target_class = np.argmax(model_output.data.numpy())\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Zero grads\n",
        "        self.model.features.zero_grad()\n",
        "        self.model.classifier.zero_grad()\n",
        "        # Backward pass with specified target\n",
        "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "        # Get hooked gradients\n",
        "        guided_gradients = self.extractor.gradients.data.numpy()[0]\n",
        "        # Get convolution outputs\n",
        "        target = conv_output.data.numpy()[0]\n",
        "        # Get weights from gradients\n",
        "        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
        "        # Create empty numpy array for cam\n",
        "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
        "        # Multiply each weight with its conv output and then, sum\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
        "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
        "        cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2],\n",
        "                       input_image.shape[3]), Image.ANTIALIAS))/255\n",
        "        return cam\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZ-Dyfmsjn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_example = 0 \n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "# Grad cam\n",
        "grad_cam = GradCam(pretrained_model, target_layer=11)\n",
        "# Generate cam mask\n",
        "cam = grad_cam.generate_cam(prep_img, target_class)\n",
        "# Save mask\n",
        "save_class_activation_images(original_image, cam, file_name_to_export)\n",
        "print('Grad cam completed')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}