{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "hw1_empty.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlelarge/dataflowr/blob/master/Notebooks/hw1_empty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djSVCMmseS2U",
        "colab_type": "text"
      },
      "source": [
        "# Homewrok 1: MLP from scratch\n",
        "\n",
        "In this homework, you will code a [Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) with one hidden layer to classify cloud of points in 2D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZyQNng_eS2X",
        "colab_type": "text"
      },
      "source": [
        "## 1. Some utilities and your dataset\n",
        "\n",
        "You should not modify the code in this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqqgR2VseS2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all of these libraries are used for plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the dataset\n",
        "def plot_data(ax, X, Y):\n",
        "    plt.axis('off')\n",
        "    ax.scatter(X[:, 0], X[:, 1], s=1, c=Y, cmap='bone')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paV_eG8teS2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, Y = make_moons(n_samples=2000, noise=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8mjvwU3eS2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "x_min, x_max = -1.5, 2.5\n",
        "y_min, y_max = -1, 1.5\n",
        "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
        "ax.set_xlim(x_min, x_max)\n",
        "ax.set_ylim(y_min, y_max)\n",
        "plot_data(ax, X, Y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUyl_bBHeS2l",
        "colab_type": "text"
      },
      "source": [
        "This is your dataset: two moons each one corresponding to one class (black or white in the picture above).\n",
        "\n",
        "In order to make it more fun and illustrative, the code below allows you to see the decision boundary of your classifier. Unfortunately, animation is not working on colab..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoWsSVqyeS2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the grid on which we will evaluate our classifier\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, .1),\n",
        "                     np.arange(y_min, y_max, .1))\n",
        "\n",
        "to_forward = np.array(list(zip(xx.ravel(), yy.ravel())))\n",
        "\n",
        "# plot the decision boundary of our classifier\n",
        "def plot_decision_boundary(ax, X, Y, classifier):\n",
        "    # forward pass on the grid, then convert to numpy for plotting\n",
        "    Z = classifier.forward(to_forward)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # plot contour lines of the values of our classifier on the grid\n",
        "    ax.contourf(xx, yy, Z>0.5, cmap='Blues')\n",
        "    \n",
        "    # then plot the dataset\n",
        "    plot_data(ax, X,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZ6xvA1eS2p",
        "colab_type": "text"
      },
      "source": [
        "## 2. MLP in numpy\n",
        "\n",
        "Here you need to code your implementation of the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation and the [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ6LtMH1eS2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyReLU(object):\n",
        "    def forward(self, x):\n",
        "        # the relu is y_i = max(0, x_i)\n",
        "        # your code here\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        # the gradient is 1 for the inputs that were above 0, 0 elsewhere\n",
        "        # your code here\n",
        "    \n",
        "    def step(self, learning_rate):\n",
        "        # no need to do anything here, since ReLU has no parameters\n",
        "        # your code here\n",
        "\n",
        "class MySigmoid(object):\n",
        "    def forward(self, x):\n",
        "        # the sigmoid is y_i = 1./(1+exp(-x_i))\n",
        "        # your code here\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        # the partial derivative is e^-x / (e^-x + 1)^2\n",
        "        # your code here\n",
        "    \n",
        "    def step(self, learning_rate):\n",
        "        # no need to do anything here since Sigmoid has no parameters\n",
        "        # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxNrlBIeS2u",
        "colab_type": "text"
      },
      "source": [
        "Probably a good time to test your functions..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ1zsgvqeS2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_relu = MyReLU()\n",
        "test_relu.forward(X[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Y4EXbUeS2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_relu.backward(1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W5OADrqeS22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sig = MySigmoid()\n",
        "test_sig.forward(np.ones(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQpgqhC4eS25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sig.backward(np.ones(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEOtDH2eS28",
        "colab_type": "text"
      },
      "source": [
        "A bit more complicated, you need now to implement your linear layer i.e. multiplication by a matrix W and summing with a bias b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HUqnv4eeS29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinear(object):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        # initialize two random matrices for W and b (use np.random.randn)\n",
        "        # your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # save a copy of x, you'll need it for the backward\n",
        "        # return Wx + b\n",
        "        # your code here\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # y_i = \\sum_j W_{i,j} x_j + b_i\n",
        "        # d y_i / d W_{i, j} = x_j\n",
        "        # d loss / d y_i = grad_output[i]\n",
        "        # so d loss / d W_{i,j} = x_j * grad_output[i]  (by the chain rule)\n",
        "        #your code here\n",
        "        \n",
        "        # d y_i / d b_i = 1\n",
        "        # d loss / d y_i = grad_output[i]\n",
        "        # your code here\n",
        "        \n",
        "        # now we need to compute the gradient with respect to x to continue the back propagation\n",
        "        # d y_i / d x_j = W_{i, j}\n",
        "        # to compute the gradient of the loss, we have to sum over all possible y_i in the chain rule\n",
        "        # d loss / d x_j = \\sum_i (d loss / d y_i) (d y_i / d x_j)\n",
        "        # your code here\n",
        "    \n",
        "    def step(self, learning_rate):\n",
        "        # update self.W and self.b in the opposite direction of the stored gradients, for learning_rate\n",
        "        # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA6lwIrxeS3A",
        "colab_type": "text"
      },
      "source": [
        "As we did in course, you need now to code your network. Recall with a Sigmoid layer, you should use the BCE loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsehcwzheS3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(object):\n",
        "    def __init__(self, layers):\n",
        "        # your code here\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # your code here\n",
        "    \n",
        "    def compute_loss(self, out, label):\n",
        "        # use the BCE loss\n",
        "        # -(label * log(output) + (1-label) * log(1-output))\n",
        "        # save the gradient, and return the loss\n",
        "        \n",
        "        # beware of dividing by zero in the gradient.\n",
        "        # split the computation in two cases, one where the label is 0 and another one where the label is 1\n",
        "        # add a small value (1e-10) to the denominator\n",
        "        # your code here\n",
        "\n",
        "    def backward(self):\n",
        "        # apply backprop sequentially, starting from the gradient of the loss\n",
        "        # your code here\n",
        "    \n",
        "    def step(self, learning_rate):\n",
        "        # take a gradient step for each layers\n",
        "        # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34an0e0eS3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h=50\n",
        "\n",
        "# define your network with your Sequential\n",
        "# it should be a linear layer with 2 inputs and h outputs, followed by a ReLU\n",
        "# then a linear layer with h inputs and 1 outputs, followed by a sigmoid\n",
        "# feel free to try other architectures\n",
        "\n",
        "net = # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6HBYWoJeS3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unfortunately animation is not working on colab\n",
        "# you can uncomment the following line if not on colab\n",
        "#%matplotlib notebook\n",
        "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
        "ax.set_xlim(x_min, x_max)\n",
        "ax.set_ylim(y_min, y_max)\n",
        "losses = []\n",
        "learning_rate = 1e-2\n",
        "for it in range(10000):\n",
        "    # pick a random example id\n",
        "    j = np.random.randint(1, len(X))\n",
        "\n",
        "    # select the corresponding example and label\n",
        "    example = X[j]\n",
        "    label = Y[j]\n",
        "\n",
        "    # do a forward pass on the example\n",
        "    # your code here\n",
        "\n",
        "    # compute the loss according to your output and the label\n",
        "    # your code here\n",
        "    \n",
        "    # backward pass\n",
        "    # your code here\n",
        "    \n",
        "    # gradient step\n",
        "    # your code here\n",
        "\n",
        "    # draw the current decision boundary every 250 examples seen\n",
        "    if it % 250 == 0 : \n",
        "        plot_decision_boundary(ax, X,Y, net)\n",
        "        fig.canvas.draw()\n",
        "plot_decision_boundary(ax, X,Y, net)\n",
        "fig.canvas.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAOZgdrfeS3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FPTiWyzeS3N",
        "colab_type": "text"
      },
      "source": [
        "## 3. Using a Pytorch module\n",
        "\n",
        "In this last part, use `toch.nn.Module` to recode `MyLinear` and `MyReLU` so that these modules will be pytorch compatible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUsYbOD5eS3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# y = Wx + b\n",
        "class MyLinear_mod(nn.Module):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super(MyLinear_mod, self).__init__()\n",
        "        # define self.A and self.b the weights and biases\n",
        "        # initialize them with a normal distribution\n",
        "        # use nn.Parameters\n",
        "        # your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "      # your code here\n",
        "      \n",
        "\n",
        "class MyReLU_mod(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyReLU_mod, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FYjrvdKeS3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the grid for plotting the decision boundary should be now made of tensors.\n",
        "to_forward = torch.from_numpy(np.array(list(zip(xx.ravel(), yy.ravel())))).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az5qbc5seS3U",
        "colab_type": "text"
      },
      "source": [
        "Define your network using `MyLinear_mod`, `MyReLU_mod` and [`nn.Sigmoid`](https://pytorch.org/docs/stable/nn.html#sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEh0d3A2eS3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h=50\n",
        "\n",
        "# define your network with nn.Sequential\n",
        "# use MyLinear_mod, MyReLU_mod and nn.Sigmoid (from pytorch)\n",
        "net = # your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUVfEkwEeS3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
        "\n",
        "X_torch = torch.from_numpy(X).float()\n",
        "Y_torch = torch.from_numpy(Y).float()\n",
        "\n",
        "# you can decomment the following line if not on colab\n",
        "#%matplotlib notebook\n",
        "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
        "ax.set_xlim(x_min, x_max)\n",
        "ax.set_ylim(y_min, y_max)\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "for it in range(10000):\n",
        "    # pick a random example id \n",
        "    j = np.random.randint(1, len(X))\n",
        "\n",
        "    # select the corresponding example and label\n",
        "    example = X_torch[j:j+1]\n",
        "    label = Y_torch[j]\n",
        "\n",
        "    # do a forward pass on the example\n",
        "    # your code here\n",
        "\n",
        "    # compute the loss according to your output and the label\n",
        "    # your code here\n",
        "\n",
        "    # zero the gradients\n",
        "    # your code here\n",
        "\n",
        "    # backward pass\n",
        "    # your code here\n",
        "\n",
        "    # gradient step\n",
        "    # your code here\n",
        "\n",
        "    # draw the current decision boundary every 250 examples seen\n",
        "    if it % 250 == 0 : \n",
        "        plot_decision_boundary(ax, X,Y, net)\n",
        "        fig.canvas.draw()\n",
        "plot_decision_boundary(ax, X,Y, net)\n",
        "fig.canvas.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_dzcL3geS3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}