{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_filled.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIz1pMbQD1R4",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlelarge/dataflowr/blob/master/CEA_EDF_INRIA/VAE_filled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdARoOAOD1R6",
        "colab_type": "text"
      },
      "source": [
        "# VAE for MNIST clustering and generation\n",
        "\n",
        "The goal of this notebook is to explore some recent works dealing with variational auto-encoder (VAE).\n",
        "\n",
        "We will use MNIST dataset and a basic VAE architecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jqTo-YBD1R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x-83sgJD1R-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghfV6JTqD1SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'data'\n",
        "\n",
        "batch_size = 128\n",
        "image_size = 784\n",
        "\n",
        "dataset = torchvision.datasets.MNIST(root=data_dir,\n",
        "                                     train=True,\n",
        "                                     transform=transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=10, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60cK2n8iWBqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_reconstruction(model, n=24):\n",
        "  x,_ = next(iter(data_loader))\n",
        "  x = x[:n,:,:,:].to(device)\n",
        "  out, _, _, log_p = model_G(x.view(-1, image_size)) \n",
        "  x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "  out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "  show(out_grid)\n",
        "  \n",
        "def plot_generation(model, n=24):\n",
        "  with torch.no_grad():\n",
        "    z = torch.randn(n, z_dim).to(device)\n",
        "    out = model.decode(z).view(-1, 1, 28, 28)\n",
        "\n",
        "  out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "  show(out_grid)\n",
        "  \n",
        "def plot_conditional_generation(model, n=8, fix_number=None):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "  \n",
        "    matrix = np.zeros((n,n_classes))\n",
        "    matrix[:,0] = 1\n",
        "\n",
        "    if fix_number is None:\n",
        "      final = matrix[:]\n",
        "      for i in range(1,n_classes):\n",
        "        final = np.vstack((final,np.roll(matrix,i)))\n",
        "      z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "      y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "      out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "    else:\n",
        "      z = torch.randn(n, z_dim).to(device)\n",
        "      y_onehot = torch.tensor(np.roll(matrix, fix_number)).type(torch.FloatTensor).to(device)\n",
        "      out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "\n",
        "  out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "  show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBa5laJbD1SD",
        "colab_type": "text"
      },
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "Consider a latent variable model with a data variable $x\\in \\mathcal{X}$ and a latent variable $z\\in \\mathcal{Z}$, $p(z,x) = p(z)p_\\theta(x|z)$. Given the data $x_1,\\dots, x_n$, we want to train the model by maximizing the marginal log-likelihood:\n",
        "\\begin{eqnarray*}\n",
        "\\mathcal{L} = \\mathbf{E}_{p_d(x)}\\left[\\log p_\\theta(x)\\right]=\\mathbf{E}_{p_d(x)}\\left[\\log \\int_{\\mathcal{Z}}p_{\\theta}(x|z)p(z)dz\\right],\n",
        "  \\end{eqnarray*}\n",
        "  where $p_d$ denotes the empirical distribution of $X$: $p_d(x) =\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}(x)$.\n",
        "\n",
        " To avoid the (often) difficult computation of the integral above, the idea behind variational methods is to instea maximize a lower bound to the log-likelihood:\n",
        "  \\begin{eqnarray*}\n",
        "\\mathcal{L} \\geq L(p_\\theta(x|z),q(z|x)) =\\mathbf{E}_{p_d(x)}\\left[\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]-\\mathrm{KL}\\left( q(z|x)||p(z)\\right)\\right].\n",
        "  \\end{eqnarray*}\n",
        "  Any choice of $q(z|x)$ gives a valid lower bound. Variational autoencoders replace the variational posterior $q(z|x)$ by an inference network $q_{\\phi}(z|x)$ that is trained together with $p_{\\theta}(x|z)$ to jointly maximize $L(p_\\theta,q_\\phi)$. The variational posterior $q_{\\phi}(z|x)$ is also called the encoder and the generative model $p_{\\theta}(x|z)$, the decoder or generator.\n",
        "\n",
        "The first term $\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ is the negative reconstruction error. Indeed under a gaussian assumption i.e. $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), 1)$ the term $\\log p_\\theta(x|z)$ reduced to $\\propto \\|x-\\mu_\\theta(z)\\|^2$, which is often used in practice. The term $\\mathrm{KL}\\left( q(z|x)||p(z)\\right)$ can be seen as a regularization term, where the variational posterior $q_\\phi(z|x)$ should be matched to the prior $p(z)= \\mathcal{N}(0,1)$.\n",
        "\n",
        "Variational Autoencoders were introduced by [Kingma and Welling](https://arxiv.org/abs/1312.6114), see also [Doersch](https://arxiv.org/abs/1606.05908) for a tutorial.\n",
        "\n",
        "There are vairous examples of VAE in pytorch available [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65). The code below is taken from this last source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMrOsX2aD1SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, image_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        mu, log_var = self.fc2(h), self.fc3(h)\n",
        "        return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return torch.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9X3lOjuD1SG",
        "colab_type": "text"
      },
      "source": [
        "Here for the loss, instead of MSE for the reconstruction loss, we take BCE. The code below is still from the pytorch tutorial (with minor modifications to avoid warnings!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBBLOKFaD1SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start training\n",
        "def train(model, data_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, _) in enumerate(data_loader):\n",
        "            \n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            x_reconst, mu, log_var = model(x)\n",
        "\n",
        "            # Compute reconstruction loss and kl divergence\n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + kl_div\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                       .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPQkfJj7D1SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model ,data_loader, num_epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UIuU-iaD1SK",
        "colab_type": "text"
      },
      "source": [
        "Let see how our network reconstructs our last batch. We display pairs of original digits and reconstructed version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK6-6E_jD1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_reconstruction(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjiCIs3LD1SO",
        "colab_type": "text"
      },
      "source": [
        "Let see now, how our network generates new samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATtfU48kD1SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_generation(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pWAbK8pD1SS",
        "colab_type": "text"
      },
      "source": [
        "Not great, but we did not train our network for long... That being said, we have no control of the generated digits. In the rest of this jupyter, we explore ways to generates zeros, ones, twos and so on. As a by product, we show how our VAE will allow us to do clustering.\n",
        "\n",
        "The main idea is to build what we call a Gumbel VAE as described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdKloPELD1SS",
        "colab_type": "text"
      },
      "source": [
        "# Gumbel VAE\n",
        "\n",
        "Implement a VAE where you add a categorical variable $c\\in \\{0,\\dots 9\\}$ so that your latent variable model is $p(c,z,x) = p(c)p(z)p_{\\theta}(x|,c,z)$ and your variational posterior is $q_{\\phi}(c|x)q_{\\phi}(z|x)$ as described in this NIPS [paper](https://arxiv.org/abs/1804.00104). Make minimal modifications to previous architecture...\n",
        "\n",
        "The idea is that you incorporates a categorical variable in your latent space. You hope that this categorical variable will encode the class of the digit, so that your network can use it for a better reconstruction. Moreover, if things work as planed, you will then be able to generate digits conditionally to the class, i.e. you can choose the class thanks to the latent categorical variable $c$ and then generate digits from this class.\n",
        "\n",
        "As noticed above, in order to sample random variables while still being able to use backpropagation required us to use the reparameterization trick which is easy for Gaussian random variables. For categorical random variables, the reparameterization trick is explained in this [paper](https://arxiv.org/abs/1611.01144). This is implemented in pytorch thanks to [F.gumbel_softmax](https://pytorch.org/docs/stable/nn.html?highlight=gumbel_softmax#torch.nn.functional.gumbel_softmax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dalEf3ND1SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CVAE model\n",
        "class VAE_Gumbel(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20, n_classes = 10):\n",
        "        super(VAE_Gumbel, self).__init__()\n",
        "        #\n",
        "        # your code here\n",
        "        #       \n",
        "        \n",
        "    def encode(self, x):\n",
        "        #\n",
        "        # your code here\n",
        "        #\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, y_onehot):\n",
        "        #\n",
        "        # your code here\n",
        "        #\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #\n",
        "        # your code here\n",
        "        #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU2K6-AYD1SV",
        "colab_type": "text"
      },
      "source": [
        "You need to modify the loss to take into account the categorical random variable with an uniform prior on $\\{0,\\dots 9\\}$, see Appendix A.2 in the NIPS [paper](https://arxiv.org/abs/1804.00104)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As4JdeVrD1SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_G(model, data_loader, num_epochs, verbose=True):\n",
        "    nmi_scores = []\n",
        "    model.train(True)\n",
        "    for epoch in range(num_epochs):\n",
        "        all_labels = []\n",
        "        all_labels_est = []\n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "\n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            #\n",
        "            # your code here\n",
        "            #\n",
        "            \n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            #\n",
        "            # your code here\n",
        "            #\n",
        "\n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + kl_div + entropy\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Entropy: {:.4f}\" \n",
        "                           .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size,\n",
        "                                   kl_div.item()/batch_size, entropy.item()/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmD7605vD1SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)\n",
        "\n",
        "train_G(model_G, data_loader, num_epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQl9oLXGD1Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_reconstruction(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr638oBuD1Sc",
        "colab_type": "text"
      },
      "source": [
        "This was for reconstruction, but we care more about generation. For each category, we may generate n=8 samples with the plot_conditional_generation() function. We expect that on each line only one digit should be represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfHC2HIJD1Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conditional_generation(model, n=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ4vuiv-D1Sh",
        "colab_type": "text"
      },
      "source": [
        "It does not look like our original idea is working...\n",
        "\n",
        "What is happening is that our network is not using the categorical variable. We can track the [normalized mutual information](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) between the true labels and the labels 'predicted' by our network (just by taking the category with maximal probability). \n",
        "\n",
        "Change your training loop to return the normalized mutual information (NMI) for each epoch. Plot the curve to check that the NMI is actually decreasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miS30SzyD1Sh",
        "colab_type": "text"
      },
      "source": [
        "This problem is explained in this [paper](https://arxiv.org/abs/1804.03599) and a solution is proposed in Section 5.\n",
        "\n",
        "In order to force our network to use the categorical variable, we will change the loss according to this [paper](https://arxiv.org/abs/1804.00104), Section 3 equation (7).\n",
        "\n",
        "Implement this change in the training loop and plot the new NMI curve. For $\\beta = 20, C_z=100, C_c=100$, you should see that NMI increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU7nAfnpD1Si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "def train_G_modified_loss(model, data_loader, num_epochs, beta=1 , C_z_fin=0, C_c_fin=0, verbose=True):\n",
        "    model.train(True)\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        C_z = C_z_fin * epoch/num_epochs\n",
        "        C_c = C_c_fin * epoch/num_epochs\n",
        "        \n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "            #\n",
        "            # your code here\n",
        "            # \n",
        "            \n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + beta * ( abs(kl_div - C_z) + abs(entropy - C_c) )\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Entropy: {:.4f}\" \n",
        "                           .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size,\n",
        "                                   kl_div.item()/batch_size, entropy.item()/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzIoxt2BD1Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "beta = 20\n",
        "C_z_fin=200\n",
        "C_c_fin=200\n",
        "\n",
        "model_G = VAE_Gumbel(z_dim = z_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)\n",
        "\n",
        "train_G_modified_loss(model_G, data_loader, num_epochs=num_epochs, beta=beta, C_z_fin=C_z_fin, C_c_fin=C_c_fin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32IgAlIeaPP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_reconstruction(model_G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DaqzgAhD1Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conditional_generation(model_G, fix_number=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFiczWhmD1Ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conditional_generation(model_G, fix_number=0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}